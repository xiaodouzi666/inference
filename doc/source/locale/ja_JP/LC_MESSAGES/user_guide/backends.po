# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-01-02 17:07+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: ja_JP\n"
"Language-Team: ja_JP <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/user_guide/backends.rst:5
msgid "Backends"
msgstr ""

#: ../../source/user_guide/backends.rst:7
msgid ""
"Xinference supports multiple backends for different models. After the "
"user specifies the model, xinference will automatically select the "
"appropriate backend."
msgstr ""

#: ../../source/user_guide/backends.rst:11
msgid "llama-cpp-python"
msgstr ""

#: ../../source/user_guide/backends.rst:12
msgid ""
"`llama-cpp-python <https://github.com/abetlen/llama-cpp-python>`_ is the "
"python binding of `llama.cpp`. `llama-cpp` is developed based on the "
"tensor library `ggml`, supporting inference of the LLaMA series models "
"and their variants."
msgstr ""

#: ../../source/user_guide/backends.rst:16
msgid ""
"We recommend that users install `llama-cpp-python` on the worker "
"themselves and adjust the `cmake` parameters according to the hardware to"
" achieve the best inference efficiency. Please refer to the `llama-cpp-"
"python installation guide <https://github.com/abetlen/llama-cpp-python"
"#installation-with-openblas--cublas--clblast--metal>`_."
msgstr ""

#: ../../source/user_guide/backends.rst:30
msgid "transformers"
msgstr ""

#: ../../source/user_guide/backends.rst:31
msgid ""
"Transformers supports the inference of most state-of-art models. It is "
"the default backend for models in PyTorch format."
msgstr ""

#: ../../source/user_guide/backends.rst:34
msgid "vLLM"
msgstr ""

#: ../../source/user_guide/backends.rst:35
msgid "vLLM is a fast and easy-to-use library for LLM inference and serving."
msgstr ""

#: ../../source/user_guide/backends.rst:37
msgid ""
"vLLM is fast with: - State-of-the-art serving throughput - Efficient "
"management of attention key and value memory with PagedAttention - "
"Continuous batching of incoming requests - Optimized CUDA kernels"
msgstr ""

#: ../../source/user_guide/backends.rst:43
msgid ""
"When the following conditions are met, Xinference will choose vLLM as the"
" inference engine:"
msgstr ""

#: ../../source/user_guide/backends.rst:45
msgid "The model format is PyTorch"
msgstr ""

#: ../../source/user_guide/backends.rst:46
msgid "The quantization method is none"
msgstr ""

#: ../../source/user_guide/backends.rst:47
msgid "The system is Linux and has at least one CUDA device"
msgstr ""

#: ../../source/user_guide/backends.rst:48
msgid "The model is within the list of models supported by vLLM."
msgstr ""

#: ../../source/user_guide/backends.rst:50
msgid "Currently, supported model includes:"
msgstr ""

#: ../../source/user_guide/backends.rst:52
msgid "``llama-2``, ``llama-2-chat``"
msgstr ""

#: ../../source/user_guide/backends.rst:53
msgid "``baichuan``, ``baichuan-chat``"
msgstr ""

#: ../../source/user_guide/backends.rst:54
msgid "``internlm``, ``internlm-20b``, ``internlm-chat``, ``internlm-chat-20b``"
msgstr ""

#: ../../source/user_guide/backends.rst:55
msgid "``vicuna-v1.3``, ``vicuna-v1.5``"
msgstr ""

#: ../../source/user_guide/backends.rst:56
msgid "``Yi``, ``Yi-chat``"
msgstr ""

#: ../../source/user_guide/backends.rst:57
msgid "``qwen-chat``"
msgstr ""

#: ../../source/user_guide/backends.rst:58
msgid "``code-llama``, ``code-llama-python``, ``code-llama-instruct``"
msgstr ""

#: ../../source/user_guide/backends.rst:59
msgid "``mistral-instruct-v0.1``"
msgstr ""

#: ../../source/user_guide/backends.rst:60
msgid "``chatglm3``"
msgstr ""

